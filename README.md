# PowerInfer_Inference_Engine
* PowerInfer is 11x faster then the llama.cpp for LLM inference.
* It works based on the technique called "Activation Loacality" and making inferences on cpu and consumer gpu resources.

## References: 
- Youtube Link: https://www.youtube.com/watch?v=jH9bAbmqi2I
- PowerInfer GitHub: https://github.com/SJTU-IPADS/PowerInfer
